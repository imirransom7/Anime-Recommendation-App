{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4db919b-5b1f-4741-a553-8844a83a5319",
   "metadata": {},
   "source": [
    "# **Machine Learning Recommendation Model**\n",
    "This model will be trained using data from user's ratings of a variety of anime\n",
    "### Imports\n",
    "Setting up the imports that will be needed for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ab18fb-ec0d-4bd3-91e6-9e08dd462ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df8925-b305-4a9c-88b5-f740f575a36c",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "### Spark Session\n",
    "Creating a spark session by creating a spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08869f21-59aa-455e-9695-9524f3a21b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName('RecAnime') \\\n",
    "            .config('spark.driver.memory', '4g') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3d5a6-9a0d-40d7-834e-c9f900924caf",
   "metadata": {},
   "source": [
    "### Spark DataFrame\n",
    "Reading the csv file, `user-score-2023`, into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaa7e7-a430-46ee-9d03-8e7c35892a7a",
   "metadata": {},
   "source": [
    "The parameter `header=True` indicates the first row of the csv file contains the column names. Setting it to True means the first row will be the header, and the columns nanmes will be inferred from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c81deb-3170-464c-8cab-d67e480b1bf0",
   "metadata": {},
   "source": [
    "The parameter `inferSchema` tells Spark to automatically infer the data types of the columns in the\n",
    "DataFrame based on the contents of the csv file. When set to True, Spark will try to determine the\n",
    "data appropriate data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a348b53-c170-4365-83d7-410edb4ee299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('E:/RhaMo/CSV Files/Anime Dataset/user-filtered.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aeca7-acef-4ed1-9b56-93cb7683fce0",
   "metadata": {},
   "source": [
    "Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f92648d-ed1e-4ddc-a803-49325d6e673f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=0, anime_id=67, rating=9),\n",
       " Row(user_id=0, anime_id=6702, rating=7),\n",
       " Row(user_id=0, anime_id=242, rating=10),\n",
       " Row(user_id=0, anime_id=4898, rating=0),\n",
       " Row(user_id=0, anime_id=21, rating=10),\n",
       " Row(user_id=0, anime_id=24, rating=9),\n",
       " Row(user_id=0, anime_id=2104, rating=0),\n",
       " Row(user_id=0, anime_id=4722, rating=8),\n",
       " Row(user_id=0, anime_id=6098, rating=6),\n",
       " Row(user_id=0, anime_id=3125, rating=9)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afff36ac-9bf3-4f38-944f-362c0b8487c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=353404, anime_id=986, rating=9),\n",
       " Row(user_id=353404, anime_id=985, rating=7),\n",
       " Row(user_id=353404, anime_id=287, rating=9),\n",
       " Row(user_id=353404, anime_id=551, rating=8),\n",
       " Row(user_id=353404, anime_id=243, rating=7),\n",
       " Row(user_id=353404, anime_id=507, rating=7),\n",
       " Row(user_id=353404, anime_id=392, rating=9),\n",
       " Row(user_id=353404, anime_id=882, rating=6),\n",
       " Row(user_id=353404, anime_id=883, rating=8),\n",
       " Row(user_id=353404, anime_id=149, rating=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9a586-371f-48c3-bf20-1efd941f962b",
   "metadata": {},
   "source": [
    "### Load Sample Data\n",
    "Since the data I am using is large, I will need to train the model with a subset of the data I have so that I can scale it up from there. I will use the sample to start off training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ddd7ac-0d5c-420d-ac0e-aaaf494e5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data.sample(fraction=0.3, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eab742-cbc8-4de6-9833-32856f5e9da2",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "The data will be split up from here. It will be split up 80/20: 80% for training the model, and 20% to test against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b675365-5cf8-4240-8dfb-d73c8e9cfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = sample_data.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689bba2-df5a-4ce2-abdf-73f7b66cf092",
   "metadata": {},
   "source": [
    "### Persisting the DataFrame\n",
    "Storing the dataframe in memory (or on disk) so that it can be reused efficiently in subsequent operations This will be useful if I am going to be using the dataframe multiple times in this spark application. It helps to avoid recomputing it from the source data each time it is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b249bebd-8042-46e6-a2d8-63450c85a5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, anime_id: int, rating: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.persist()\n",
    "test_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db17f36-a12d-433b-8b60-dc76ae4d9a1b",
   "metadata": {},
   "source": [
    "### Repartitioning the DataFrame\n",
    "I am reshuffling the data in the DataFrame by changing the distribution of data across partitions. Partitions are smaller units of data that Spark uses to distribute work across nodes in a cluster. The amount of partitions affects the parallelism and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d4512c-a17f-4c4c-8147-a1f4005d0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.repartition(30)\n",
    "test_data = test_data.repartition(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b5231-9de0-4aeb-92d0-5d1ed6d75e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

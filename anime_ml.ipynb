{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4db919b-5b1f-4741-a553-8844a83a5319",
   "metadata": {},
   "source": [
    "# **ALS Machine Learning Recommendation Model**\n",
    "This model will be trained using data from user's ratings of a variety of anime\n",
    "### Imports\n",
    "Setting up the imports that will be needed for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ab18fb-ec0d-4bd3-91e6-9e08dd462ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df8925-b305-4a9c-88b5-f740f575a36c",
   "metadata": {},
   "source": [
    "## **Preparation For Model Training**\n",
    "### Spark Session\n",
    "Creating a spark session by creating a spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08869f21-59aa-455e-9695-9524f3a21b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName('RecAnime') \\\n",
    "            .config('spark.driver.memory', '8g') \\\n",
    "            .config('spark.sql.pivotMaxValues', 20000) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3d5a6-9a0d-40d7-834e-c9f900924caf",
   "metadata": {},
   "source": [
    "### Spark DataFrame\n",
    "Reading the csv file, `user-score-2023`, into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaa7e7-a430-46ee-9d03-8e7c35892a7a",
   "metadata": {},
   "source": [
    "The parameter `header=True` indicates the first row of the csv file contains the column names. Setting it to True means the first row will be the header, and the columns nanmes will be inferred from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c81deb-3170-464c-8cab-d67e480b1bf0",
   "metadata": {},
   "source": [
    "The parameter `inferSchema` tells Spark to automatically infer the data types of the columns in the\n",
    "DataFrame based on the contents of the csv file. When set to True, Spark will try to determine the\n",
    "data appropriate data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a348b53-c170-4365-83d7-410edb4ee299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('E:/RhaMo/CSV Files/Anime Dataset/user-filtered.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e280dd-cc16-4051-a0ae-4c90610ccf1f",
   "metadata": {},
   "source": [
    "Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ede2271-a783-41d3-b555-25cdb91f5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('user_id', 'userId') \\\n",
    "       .withColumnRenamed('anime_id', 'itemId') \\\n",
    "       .withColumnRenamed('rating', 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aeca7-acef-4ed1-9b56-93cb7683fce0",
   "metadata": {},
   "source": [
    "Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f92648d-ed1e-4ddc-a803-49325d6e673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|userId|itemId|rating|\n",
      "+------+------+------+\n",
      "|     0|    67|     9|\n",
      "|     0|  6702|     7|\n",
      "|     0|   242|    10|\n",
      "|     0|  4898|     0|\n",
      "|     0|    21|    10|\n",
      "|     0|    24|     9|\n",
      "|     0|  2104|     0|\n",
      "|     0|  4722|     8|\n",
      "|     0|  6098|     6|\n",
      "|     0|  3125|     9|\n",
      "+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9a586-371f-48c3-bf20-1efd941f962b",
   "metadata": {},
   "source": [
    "### Load Sample Data\n",
    "Since the data I am using is large, I will need to train the model with a subset of the data I have so that I can scale it up from there. I will use the sample to start off training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ddd7ac-0d5c-420d-ac0e-aaaf494e5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data.sample(fraction=0.4, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eab742-cbc8-4de6-9833-32856f5e9da2",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "The data will be split up from here. It will be split up 80/20: 80% for training the model, and 20% to test against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b675365-5cf8-4240-8dfb-d73c8e9cfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = sample_data.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689bba2-df5a-4ce2-abdf-73f7b66cf092",
   "metadata": {},
   "source": [
    "### Persisting the DataFrame\n",
    "Storing the dataframe in memory (or on disk) so that it can be reused efficiently in subsequent operations This will be useful if I am going to be using the dataframe multiple times in this spark application. It helps to avoid recomputing it from the source data each time it is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249bebd-8042-46e6-a2d8-63450c85a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.persist()\n",
    "test_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db17f36-a12d-433b-8b60-dc76ae4d9a1b",
   "metadata": {},
   "source": [
    "### Repartitioning the DataFrame\n",
    "I am reshuffling the data in the DataFrame by changing the distribution of data across partitions. Partitions are smaller units of data that Spark uses to distribute work across nodes in a cluster. The amount of partitions affects the parallelism and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4512c-a17f-4c4c-8147-a1f4005d0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.repartition(30)\n",
    "test_data = test_data.repartition(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce3cb7-c29c-476c-81d6-6c889a0ebaad",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "The trainning of the model is underway here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39625d68-5b7e-4352-9b61-752373ca5a9b",
   "metadata": {},
   "source": [
    "### Range of Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b200b1-8755-49fa-96c9-12e07669a483",
   "metadata": {},
   "source": [
    "`rank` controls the number of latent factors (also known as embeddings) used in the matrix factorization process. Larger values of rank allow the model to capture more complex relationships but may lead to overfitting if set too high. Smaller values of rank simplify the model but may result in underfitting. Range: It's common to experiment with values like [5, 10, 20, 50, 100] to find an appropriate balance between complexity and performance. maxIter:ing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adfdd3-84dc-456d-ab6c-dfec3f8e87a7",
   "metadata": {},
   "source": [
    "`maxIter` specifies the maximum number of iterations or updates the ALS algorithm performs. Increasing maxIter allows the algorithm to potentially converge to a better solution but may also increase computation time. Range: You can typically explore values like [5, 10, 20, 50, 100] depending on your dataset and computational resources. regParam (or lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1188c9-6630-4dd5-b1cf-9b34fc094b9c",
   "metadata": {},
   "source": [
    "`regParam` controls the amount of regularization applied to the model. Regularization helps prevent overfitting by penalizing large values in the latent factor matrices. Smaller values of regParam result in less regularization and may lead to overfitting. Larger values of regParam increase regularization and may result in underfitting. Range: You can explore a range of values, such as [0.01, 0.1, 0.5, 1.0], to find the right balance between fitting the training data and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac03ca-7bb2-4769-b078-d9b33b90ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    rank=50,\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    userCol='userId',\n",
    "    itemCol='itemId',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740778b-1750-457d-ab65-152c5ec68f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"E:/RhaMo/CSV Files/Anime Dataset/user-filtered.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f688706-be01-45d7-81ef-ea5db6b07b4a",
   "metadata": {},
   "source": [
    "Training the trainig data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219527e-e1d1-4c49-9b69-6cddff11f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bf92a-0de2-4ab2-80d2-353fd9fa8bd6",
   "metadata": {},
   "source": [
    "Getting the prediciton from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968dd78a-620a-4285-ac0a-875f94e5f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd37b5-0432-45d2-b1a8-aae5ed94026a",
   "metadata": {},
   "source": [
    "### **Evaluate the model using the Regression Evaluator**\n",
    "- `metricName`: 'rsme' indicates that the evaluation metric is Root Mean Squared Error (RSME)gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166a1f6-5c01-425b-b739-e60634a4599f",
   "metadata": {},
   "source": [
    "- `labelCol`: 'rating' specifies the column in the DataFrame containg the actual ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450636c-d10e-441f-b619-98eb2c02032e",
   "metadata": {},
   "source": [
    "- `predictionCol`: 'prediction' specifies the column in the DataFrame containing the predicited ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a48138-5f3e-4cc0-9baa-6fbcbf341a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='rating',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "rsme = evaluator.evaluate(predictions)\n",
    "print(f'Root Mean Squared Error on test data: {rsme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184880c-e6d1-42e7-b727-a951f4685777",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Making a baseline model to compare with my trined model to see how good the predicitions are compared to the baseline. If lower than the baseline, this shows that my model has picked up on a pattern with it's algorithm. It will provide a reference point for comparing the performance of my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fc0c9-8856-4379-8eda-605769232681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3db728-9d2d-4378-8524-d022700ad1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/RhaMo/CSV Files/Anime Dataset/user-filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee426bb1-21ce-4a13-b7be-2742df025356",
   "metadata": {},
   "source": [
    "### Calculations\n",
    "- We calculate the mean rating using the .mean() method of the DataFrame\r",
    "- \n",
    "We create an array of baseline predictions using np.full_like() to fill it with the mean rating\n",
    "-  \r\n",
    "We use the mean_squared_error function from the sklearn.metrics module to calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c764d-6e1b-40fa-ba6d-255ad5531d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the Mean\n",
    "mean_rating = df['rating'].mean()\n",
    "\n",
    "# Step 2: Make Predictions\n",
    "baseline_predictions = np.full_like(df['rating'], fill_value=mean_rating)\n",
    "\n",
    "# Step 3: Compute RMSE\n",
    "baseline_rmse = np.sqrt(mean_squared_error(df['rating'], baseline_predictions))\n",
    "print(\"Baseline RMSE:\", baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54599e70-24d0-4d7a-b8d5-1c9be5045c9f",
   "metadata": {},
   "source": [
    "# **User-Based Collaboritve Filtering (USBF)**\n",
    "I am going to train with a different alogrithm to see if it produces better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62328da3-ea8d-4547-a2a1-e4b746636bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776d304-4433-4d68-90a6-0e0f6db28d00",
   "metadata": {},
   "source": [
    "Checking the number of unique anime_id to set spark.sql.pivotMaxValues the the at least nummber of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338f805-3491-47af-ba5a-925cc0213bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anime_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fcde3b-1b0a-4fdb-abbb-a83b439fbc19",
   "metadata": {},
   "source": [
    "Converting user-item interactions to a user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35b1921-9ebd-441f-b776-abb2aaa3671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = sample_data.groupBy('userId').pivot('itemId').agg(F.avg('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbea2190-97cd-4760-9122-adabd1209e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate user similarities\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\")\n",
    "assembler = VectorAssembler(inputCols=pivot_df.columns[1:], outputCol=\"features\")\n",
    "vector_df = assembler.transform(pivot_df).select('userId', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32422d9-e3c6-43ad-9a46-d38135f73d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(v1, v2):\n",
    "    return sum(x * y for x, y in zip(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bdbbea6-c1b4-4a7d-a25e-d1c6714c3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dot_product function as a UDF\n",
    "dot_product_udf = F.udf(dot_product)\n",
    "\n",
    "# Alias the columns in one of the DataFrames to resolve ambiguity\n",
    "vector_df = vector_df.alias(\"vector1\")\n",
    "# cross_joined_df = vector_df.crossJoin(vector_df.alias(\"vector2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a6e22e9-85cc-43ce-af32-8dd69a175cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = vector_df.crossJoin(vector_df.alias(\"vector2\")) \\\n",
    "    .withColumn(\"dot_product\", dot_product_udf(F.col(\"vector1.features\"), F.col(\"vector2.features\"))) \\\n",
    "    .withColumn(\"norm_dot_product\", F.col(\"dot_product\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5655b3-db5a-4224-8361-9508e0d33a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = similarity_df.select(\"vector1.userId\", \"vector2.userId\", \"norm_dot_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be2c068-c664-4eb3-8d79-010be6ee85ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (928742355.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    .orderBy(F.col('dot_product').desc()\u001b[0m\n\u001b[1;37m                                        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "similar_users = similarity_df.filter((F.col('userId') != F.col('userId')) & (F.col('dot_product') > threshold))\\\n",
    "                    .orderBy(F.col('dot_product').desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266aa56-2c20-4175-876c-bea31fe8c837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
